# NTHU CS542200 Parallel Programming Homework 1: Odd-Even Sort
## 112062591 李威辰

## Implementation
此作業為利用MPI實作 odd-even sort algorithm 的平行化處理，由以下步驟組成:

1. ***MPI初始化*** : 做任何利用MPI的程式碼都必須要做初始化，允許程式進行平行計算，並用參數去保存每個process_id和processes的數量。
```MPI=
MPI_Init(&argc, &argv);
int rank, size;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
MPI_Comm_size(MPI_COMM_WORLD, &size);
```
* rank：目前執行中process的編號，每個都會有一個唯一的rank。
* size：processes的總數。
2. ***資料分割***：MPI是利用message passing的方式來達成每個thread之間的資料傳遞，所以在執行主要程式必須要給每個thread初始資料
```MPI=
int remainder = n % size;
int data_size = n / size;
if (rank < remainder)
    data_size++;
int ln_data_size = data_size;
if (rank == remainder)
    ln_data_size++;
int rn_data_size = data_size;
if (rank == remainder - 1)
    rn_data_size--;
```
* remainder：n除以執行緒總數的餘數，用來處理當資料無法均勻分配時，**rank前0到remainder-1的持有資料會多一**。
* data_size：每個process處理的資料數。如果rank前0到remainder-1資料量會再多一個。
* ln_data_size : 左邊process的資料量。若此process的rank剛好等於remainder，代表左邊process的**rank小於remainder**，所以資料量要**加一**。
* rn_data_size : 右邊process的資料量。若此process的rank剛好等於remainder-1，代表右邊process的**rank大於等於remainder**，所以資料量要**減一**。
3. ***定位每個process的資料區段***：計算每個process應該從哪個位置開始讀取資料。
```MPI=
int data_location = n / size * rank;
if (rank < remainder)
    data_location += rank;
else
    data_location += remainder;
```
* data_location : 開始讀取資料的位置。
    * 若process的**rank小於remainder**，代表它之前每個process都拿了**n / size+1**個資料，所以輪到此process拿到資料的位置為(n / size+1) * rank = n / size * rank + rank。
    * 若process的**rank大於等於remainder**，代表它之前process **rank小於remainder都拿了n / size+1**個資料且一**共remainder個process**，process **rank大於等於remainder都拿了n / size**個資料，所以輪到此process拿到資料的位置為
(n / size + 1) * remainder + (rank - remainder) * n / size 
= n / size * rank + remainder。
4. ***初始化緩衝區並讀取數據*** : 使用MPI_File_open和MPI_File_read_at來從檔案讀取每個process的資料到buffer。
```MPI=
float *buffer = new float[data_size], *rn_buffer = new float[rn_data_size], *ln_buffer = new float[ln_data_size], *tp_buffer = new float[data_size];

MPI_File_open(MPI_COMM_WORLD, input_filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &input_file);
MPI_File_read_at(input_file, sizeof(float) * data_location, buffer, data_size, MPI_FLOAT, MPI_STATUS_IGNORE);
MPI_File_close(&input_file);
```
* buffer：儲存當前process的資料。
* rn_buffer：用來儲存從右邊process的資料。
* ln_buffer：用來儲存從左邊process的資料。
* tp_buffer：暫時用來存交換排序結果的buffer。
5. **初始排序** : process的資料進行初始排序以便之後的交換。
```MPI=
sort(buffer, buffer + data_size);
```
6. ***process位置初始化*** : 初始化設定process的位置，此作業要和鄰居的process進行資料交換，且這裡訂下的方式是process的位置在左邊，該process就一定和右邊的互換，反之亦然。
```MPI=
 int position = 0; // left 0, right 1;
if (rank % 2)
    position = 1;
```
* position : 該process位置。等於0左邊或等於1右邊。
7. ***odd-even-sort主要運作*** : 利用迴圈去跑process資料互換的過程，此迴圈的次數為processes總數加一次(size + 1)，因為odd-even-sort可以簡單理解為普遍bubble-sort的平行版本，所以迴圈執行的次數會只剩約size次，所以這邊取它最多跑約size + 1次即可完成。以下為完整程式和說明 :
```MPI=
int it = size + 1;
while (it--)
    {
        if (position == 0 && data_size > 0 && rank != size - 1 && rn_data_size > 0) // give to right
        {
            MPI_Sendrecv(buffer+data_size-1, 1, MPI_FLOAT, rank + 1, 0, rn_buffer, 1, MPI_FLOAT, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            if(buffer[data_size-1]>rn_buffer[0])
            {
                MPI_Sendrecv(buffer, data_size-1, MPI_FLOAT, rank + 1, 0, rn_buffer+1, rn_data_size-1, MPI_FLOAT, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
                int id = 0, rb_id = 0, tp_id = 0;
                while (id < data_size && rb_id < rn_data_size && tp_id < data_size)
                {
                    if (buffer[id] < rn_buffer[rb_id])
                    {
                        tp_buffer[tp_id] = buffer[id];
                        id++;
                        tp_id++;
                    }
                    else
                    {
                        tp_buffer[tp_id] = rn_buffer[rb_id];
                        rb_id++;
                        tp_id++;
                    }
                }
                if (tp_id < data_size && rb_id == rn_data_size)
                {
                    while (tp_id < data_size)
                    {
                        tp_buffer[tp_id] = buffer[id];
                        tp_id++;
                        id++;
                    }
                }
                swap(tp_buffer, buffer);
            }
        }
        else if (position == 1 && data_size > 0 && rank != 0 && ln_data_size > 0)
        {
            MPI_Sendrecv(buffer, 1, MPI_FLOAT, rank - 1, 0, ln_buffer+ln_data_size-1, 1, MPI_FLOAT, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            if(buffer[0]<ln_buffer[ln_data_size-1])
            {
                MPI_Sendrecv(buffer+1, data_size-1, MPI_FLOAT, rank - 1, 0, ln_buffer, ln_data_size-1, MPI_FLOAT, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
                int id = data_size - 1, lb_id = ln_data_size - 1, tp_id = data_size - 1;
                while (id > -1 && lb_id > -1 && tp_id > -1)
                {
                    if (buffer[id] > ln_buffer[lb_id])
                    {
                        tp_buffer[tp_id] = buffer[id];
                        id--;
                        tp_id--;
                    }
                    else
                    {
                        tp_buffer[tp_id] = ln_buffer[lb_id];
                        lb_id--;
                        tp_id--;
                    }
                }
                swap(tp_buffer, buffer);
            }
        }
        if (position == 0)
            position = 1;
        else
            position = 0;
    }
```
* **利用position區分位置** : 進入迴圈後，每次迭代要先決定這次的位置是在左邊還是右邊，並檢查process和鄰居的資料量是否大於零，以及若process在左邊且其rank剛好等於size - 1就不用換(因為左邊的資料一定要給右邊)，反之若process在右邊且其rank剛好等於就不用換。
```MPI=
if (position == 0 && data_size > 0 && rank != size - 1 && rn_data_size > 0) // give to right
else if (position == 1 && data_size > 0 && rank != 0 && ln_data_size > 0) // give to left
```
* ***檢查是否進行交換*** : 由於MPI程式執行的時間大多數運在資料傳遞上，所以若我們預先檢查資料是否要傳輸可以省下一些時間。
    * 若process在左邊，則若它的資料之最大值比右邊資料的最小值還小或一樣，則不需進行之後的傳遞工作。
    * 若process在右邊，則若它的資料之最小值比左邊資料的最大值還大或一樣，則不需進行之後的傳遞工作。
    * 利用MPI_Sendrecv為一種non-blocking的傳輸方式可以不用等待接收信號就進行下一步驟，可以有效節省時間。
```MPI=
// give to right
MPI_Sendrecv(buffer+data_size-1, 1, MPI_FLOAT, rank + 1, 0, rn_buffer, 1, MPI_FLOAT, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
if(buffer[data_size-1]>rn_buffer[0]){// do exchange}
// give to left
MPI_Sendrecv(buffer, 1, MPI_FLOAT, rank - 1, 0, ln_buffer+ln_data_size-1, 1, MPI_FLOAT, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
if(buffer[0]<ln_buffer[ln_data_size-1]){// do exchange}
```
* ***左右process進行資料互換*** : 先用MPI_Sendrecv進行資料互換儲存在各自的buffer中(rn_buffer或ln_buffer)，然後左右兩鄰居用比較自己每個資料的大小進行互換。
* 若process在左邊，兩鄰邊的資料從左而右看，把數字小的放到temp buffer儲存，這裡有一個條件需要判斷，有一個狀況是因為左邊proceess的資料數量一定大於等於右邊的，所以有可能發生右邊process的資料檢查到底了但temp buffer的資料數還是不夠，所以要再繼續將左邊process的資料繼續儲存在temp buffer中，然後再將temp buffer的資料跟原本自己的作互換。

```MPI=
MPI_Sendrecv(buffer, data_size-1, MPI_FLOAT, rank + 1, 0, rn_buffer+1, rn_data_size-1, MPI_FLOAT, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
int id = 0, rb_id = 0, tp_id = 0;
while (id < data_size && rb_id < rn_data_size && tp_id < data_size)
{
    if (buffer[id] < rn_buffer[rb_id])
    {
        tp_buffer[tp_id] = buffer[id];
        id++;
        tp_id++;
    }
    else
    {
        tp_buffer[tp_id] = rn_buffer[rb_id];
        rb_id++;
        tp_id++;
    }
}
if (tp_id < data_size && rb_id == rn_data_size)
{
    while (tp_id < data_size)
    {
        tp_buffer[tp_id] = buffer[id];
        tp_id++;
        id++;
    }
}
swap(tp_buffer, buffer);
```
* 若process在右邊，兩鄰邊的資料從右而左看，把數字大的放到temp buffer儲存，這裡不須條件判斷，因為左邊proceess的資料數量一定大於等於右邊的，然後再將temp buffer的資料跟原本自己的作互換。
    
```MPI=
MPI_Sendrecv(buffer+1, data_size-1, MPI_FLOAT, rank - 1, 0, ln_buffer, ln_data_size-1, MPI_FLOAT, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
int id = data_size - 1, lb_id = ln_data_size - 1, tp_id = data_size - 1;
while (id > -1 && lb_id > -1 && tp_id > -1)
{
    if (buffer[id] > ln_buffer[lb_id])
    {
        tp_buffer[tp_id] = buffer[id];
        id--;
        tp_id--;
    }
    else
    {
        tp_buffer[tp_id] = ln_buffer[lb_id];
        lb_id--;
        tp_id--;
    }
}
swap(tp_buffer, buffer);
```
* ***位置換邊*** : 依據odd-event-sort的規則，要進行基偶phase的交換，依照這邊左邊和右邊一定是互換的規則下，將process的位置移動從左到右或右到左，接著進行下一個迴圈的迭代。
```MPI=
if (position == 0)
    position = 1;
else
    position = 0;
```
8. ***寫入排序後的資料*** : 依照原本每個process自己的data_location寫入資料。
```MPI=
MPI_File_open(MPI_COMM_WORLD, output_filename, MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &output_file);
MPI_File_write_at(output_file, sizeof(float) * data_location, buffer, data_size, MPI_FLOAT, MPI_STATUS_IGNORE);
MPI_File_close(&output_file);
```
9. ***結束MPI環境*** : 
```MPI=
MPI_Finalize();
```
## Experiment & Analysis
以下測試都是在本課程提供的Apollo platform，以及用NVIDIA Nsight System 做 performance analysis。利用nvtx push pop 插入函數計算時間，下列為大致插入位置:
I/O time: 在MPI_File函數前後插入。
communication time: 在MPI_send、receive前後插入。
computation time: 執行相鄰兩process資料交換的程式前後插入。

1. **測試程式主要花時間執行的任務(Bottleneck)** : 想要去了解本次odd-even-sort的程式主要花在computation time, communication time or I/O time。下圖為rank=0的process在以下測資的執行時間圖(這裡先用Blocking Send-recev版本) : 
testcase36
n : 536869888
processes : 2, 4, 8, 12 
node : 1
![Blocking, 1 nodes, rank_0, testcase36](https://hackmd.io/_uploads/HJgP020xke.png)
testcase36
n : 536869888
processes : 8, 16, 24, 32 
node : 4
![Blocking, 4 nodes, rank_0, testcase36](https://hackmd.io/_uploads/SkdLtJebke.png)

此處communication time是花最多時間的，可以發現總運行時間並沒有隨著process數量變多而減少，只有computation time有減少，因為process的數量增多會導致彼此溝通的時間變長，導致整體時間反而變慢，所以之後我將程式碼改成 nonblocking Send-recev版本，下圖為rank=0的process在以下測資的執行時間圖 : 
testcase36
n : 536869888
processes : 2, 4, 8, 12 
node : 1
 ![Nonblocking, 1 nodes, rank_0, testcase36](https://hackmd.io/_uploads/rynyYp0lye.png)
testcase36
n : 536869888
processes : 8, 16, 24, 32 
node : 4
![nonBlocking, 4 nodes, rank_0, testcase36](https://hackmd.io/_uploads/S1iJqkl-ke.png)


 可以發現communication time有較顯著的減少，因為Send不用再等receive的接收信號可以直接做其他事情，由圖可以發現總體的時間也有因為process的數量增加而減少，所以這裡採用Nonblocking的溝通方式較為優秀。
 node : 1
 ![Speedup_ blocking-nonblocking](https://hackmd.io/_uploads/BJW-tTRlkg.png)
 node : 4
 ![Speedup_blocking-nonblocking-2](https://hackmd.io/_uploads/ByXpK1e-ke.png)

2. **測試程式在對交換資料的影響** : 
上面再解釋implementation有提到若在執行兩個process資料交換時若先判斷資料是否可以不需交換(左邊都小於等於右邊)，可以減少程式執行交換資料的時間以及傳輸溝通資料的時間，以下為兩個版本的比較(皆為nonblocking):
測資:
testcase36
n : 536869888
processes : 2, 4, 8, 12 
node : 1
**不判斷直接執行交換**:
![nobranch, 1 nodes, rank_0, testcase36](https://hackmd.io/_uploads/rJ892p0eyl.png)
**判斷是否執行交換**:
![branch, 1 nodes, rank_0, testcase36](https://hackmd.io/_uploads/By-A3p0gye.png)
testcase36
n : 536869888
processes : 8, 16, 24, 32 
node : 4
**不判斷直接執行交換**:
![nobranch, 4 nodes, rank_0, testcase36](https://hackmd.io/_uploads/SyzB91xbJe.png)
**判斷是否執行交換**:
![branch, 4 nodes, rank_0, testcase36](https://hackmd.io/_uploads/rk189yxZ1l.png)

由圖可以明顯得知communication time和computation time都可以有運行時間上的減少，所以有判斷的方法較為優秀。
node : 1
![Speedup_nobranch-branch](https://hackmd.io/_uploads/SyE3JRAeJg.png)
node : 4
![Speedup_nobranch-branch-2](https://hackmd.io/_uploads/Sypocyebkx.png)

3. **程式的擴展性(Scalability)** : 
這裡要探討此程式是否在Input資料越多的情況下運行時間會不會太久(可能成指數增加)，比較探討2.(**測試程式在對交換資料的影響**)中的兩個不同程式比較，下圖為測驗結果:
process:8
Node:1
testcase: 10, n=63942
testcase: 25, n=400009
testcase: 30, n=64123483
testcase: 40, n=536869888
**不判斷直接執行交換**:
![Scalability, nobranch,1 node, rank_0](https://hackmd.io/_uploads/Sycm1MxZkg.png)
**判斷是否執行交換**:
![Scalability, branch,1 node, rank_0](https://hackmd.io/_uploads/BkMw1zx-Je.png)
**Speedup**:
![Speedup_nobranch-branch, node 1](https://hackmd.io/_uploads/H1lTyGx-Jx.png)

process:32
Node:4
testcase: 10, n=63942
testcase: 25, n=400009
testcase: 30, n=64123483
testcase: 40, n=536869888
**不判斷直接執行交換**:
![Scalability, nobranch,4 node, rank_0](https://hackmd.io/_uploads/rJmFJMxWkl.png)
**判斷是否執行交換**:
![Scalability,branch, 4 node, rank_0](https://hackmd.io/_uploads/ByQckGeZkx.png)
**Speedup**:
![Speedup_nobranch=branch, node 4](https://hackmd.io/_uploads/HJo6Jzebke.png)
由Speedup發現，兩者的總體運行的時間是差不多的，有"判斷是否執行交換"的程式總體時間並沒有比較好，有可能因為IO運作其實由柱狀圖來看占了非常多的時間，而有"判斷是否執行交換"的優點是減少communication和computationtime(可由上圖數值比較)，所以整體的提升有限，導致兩者的scalability差不多。
## Conclusion
本次作業讓我學習到了MPI如何去傳遞資料溝通，並且也去了解了blocking和nonblocking資料溝通的實際差異，讓我可以更清楚的知道整個MPI程式的架構運作模式以及其中需要注意的實作小細節像是邊界process的資料切割處裡。本次作業的難點為在進行優化的時候結果不如預期，像是process或Node的數量上升其實程式應該要有滿顯著的效能提升，可是由上述一些實驗結果中間會因為process或Node的數量上升造成communication time增加抑或是一些IO運作的時間隨著Input增加而變很多，這些原因都導致優化實驗的結果不如預期，所以其實要優化整體程式其實是一件非常不容易的事情，希望以後我可以越來越進步。











