# NTHU CS542200 Parallel Programming Homework 1: Odd-Even Sort
## 112062591 李威辰

## Implementation
此作業為利用MPI實作 odd-even sort algorithm 的平行化處理，由以下步驟組成:

1. ***MPI初始化*** : 做任何利用MPI的程式碼都必須要做初始化，允許程式進行平行計算，並用參數去保存每個process_id和processes的數量。
```MPI=
MPI_Init(&argc, &argv);
int rank, size;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
MPI_Comm_size(MPI_COMM_WORLD, &size);
```
* rank：目前執行中process的編號，每個都會有一個唯一的rank。
* size：processes的總數。
2. ***資料分割***：MPI是利用message passing的方式來達成每個thread之間的資料傳遞，所以在執行主要程式必須要給每個thread初始資料
```MPI=
int remainder = n % size;
int data_size = n / size;
if (rank < remainder)
    data_size++;
int ln_data_size = data_size;
if (rank == remainder)
    ln_data_size++;
int rn_data_size = data_size;
if (rank == remainder - 1)
    rn_data_size--;
```
* remainder：n除以執行緒總數的餘數，用來處理當資料無法均勻分配時，**rank前0到remainder-1的持有資料會多一**。
* data_size：每個process處理的資料數。如果rank前0到remainder-1資料量會再多一個。
* ln_data_size : 左邊process的資料量。若此process的rank剛好等於remainder，代表左邊process的**rank小於remainder**，所以資料量要**加一**。
* rn_data_size : 右邊process的資料量。若此process的rank剛好等於remainder-1，代表右邊process的**rank大於等於remainder**，所以資料量要**減一**。
3. ***定位每個process的資料區段***：計算每個process應該從哪個位置開始讀取資料。
```MPI=
int data_location = n / size * rank;
if (rank < remainder)
    data_location += rank;
else
    data_location += remainder;
```
* data_location : 開始讀取資料的位置。
    * 若process的**rank小於remainder**，代表它之前每個process都拿了**n / size+1**個資料，所以輪到此process拿到資料的位置為(n / size+1) * rank = n / size * rank + rank。
    * 若process的**rank大於等於remainder**，代表它之前process **rank小於remainder都拿了n / size+1**個資料且一**共remainder個process**，process **rank大於等於remainder都拿了n / size**個資料，所以輪到此process拿到資料的位置為
(n / size + 1) * remainder + (rank - remainder) * n / size 
= n / size * rank + remainder。
4. ***初始化緩衝區並讀取數據*** : 使用MPI_File_open和MPI_File_read_at來從檔案讀取每個process的資料到buffer。
```MPI=
float *buffer = new float[data_size], *rn_buffer = new float[rn_data_size], *ln_buffer = new float[ln_data_size], *tp_buffer = new float[data_size];

MPI_File_open(MPI_COMM_WORLD, input_filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &input_file);
MPI_File_read_at(input_file, sizeof(float) * data_location, buffer, data_size, MPI_FLOAT, MPI_STATUS_IGNORE);
MPI_File_close(&input_file);
```
* buffer：儲存當前process的資料。
* rn_buffer：用來儲存從右邊process的資料。
* ln_buffer：用來儲存從左邊process的資料。
* tp_buffer：暫時用來存交換排序結果的buffer。
5. **初始排序** : process的資料進行初始排序以便之後的交換。
```MPI=
sort(buffer, buffer + data_size);
```
6. ***process位置初始化*** : 初始化設定process的位置，此作業要和鄰居的process進行資料交換，且這裡訂下的方式是process的位置在左邊，該process就一定和右邊的互換，反之亦然。
```MPI=
 int position = 0; // left 0, right 1;
if (rank % 2)
    position = 1;
```
* position : 該process位置。等於0左邊或等於1右邊。
7. ***odd-even-sort主要運作*** : 利用迴圈去跑process資料互換的過程，此迴圈的次數為processes總數加一次(size + 1)，因為odd-even-sort可以簡單理解為普遍bubble-sort的平行版本，所以迴圈執行的次數會只剩約size次，所以這邊取它最多跑約size + 1次即可完成。以下為完整程式和說明 :
```MPI=
int it = size + 1;
while (it--)
    {
        if (position == 0 && data_size > 0 && rank != size - 1 && rn_data_size > 0) // give to right
        {
            MPI_Sendrecv(buffer+data_size-1, 1, MPI_FLOAT, rank + 1, 0, rn_buffer, 1, MPI_FLOAT, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            if(buffer[data_size-1]>rn_buffer[0])
            {
                MPI_Sendrecv(buffer, data_size-1, MPI_FLOAT, rank + 1, 0, rn_buffer+1, rn_data_size-1, MPI_FLOAT, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
                int id = 0, rb_id = 0, tp_id = 0;
                while (id < data_size && rb_id < rn_data_size && tp_id < data_size)
                {
                    if (buffer[id] < rn_buffer[rb_id])
                    {
                        tp_buffer[tp_id] = buffer[id];
                        id++;
                        tp_id++;
                    }
                    else
                    {
                        tp_buffer[tp_id] = rn_buffer[rb_id];
                        rb_id++;
                        tp_id++;
                    }
                }
                if (tp_id < data_size && rb_id == rn_data_size)
                {
                    while (tp_id < data_size)
                    {
                        tp_buffer[tp_id] = buffer[id];
                        tp_id++;
                        id++;
                    }
                }
                swap(tp_buffer, buffer);
            }
        }
        else if (position == 1 && data_size > 0 && rank != 0 && ln_data_size > 0)
        {
            MPI_Sendrecv(buffer, 1, MPI_FLOAT, rank - 1, 0, ln_buffer+ln_data_size-1, 1, MPI_FLOAT, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            if(buffer[0]<ln_buffer[ln_data_size-1])
            {
                MPI_Sendrecv(buffer+1, data_size-1, MPI_FLOAT, rank - 1, 0, ln_buffer, ln_data_size-1, MPI_FLOAT, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
                int id = data_size - 1, lb_id = ln_data_size - 1, tp_id = data_size - 1;
                while (id > -1 && lb_id > -1 && tp_id > -1)
                {
                    if (buffer[id] > ln_buffer[lb_id])
                    {
                        tp_buffer[tp_id] = buffer[id];
                        id--;
                        tp_id--;
                    }
                    else
                    {
                        tp_buffer[tp_id] = ln_buffer[lb_id];
                        lb_id--;
                        tp_id--;
                    }
                }
                swap(tp_buffer, buffer);
            }
        }
        if (position == 0)
            position = 1;
        else
            position = 0;
    }
```
* **利用position區分位置** : 進入迴圈後，每次迭代要先決定這次的位置是在左邊還是右邊，並檢查process和鄰居的資料量是否大於零，以及若process在左邊且其rank剛好等於size - 1就不用換(因為左邊的資料一定要給右邊)，反之若process在右邊且其rank剛好等於就不用換。
```MPI=
if (position == 0 && data_size > 0 && rank != size - 1 && rn_data_size > 0) // give to right
else if (position == 1 && data_size > 0 && rank != 0 && ln_data_size > 0) // give to left
```
* ***檢查是否進行交換*** : 由於MPI程式執行的時間大多數運在資料傳遞上，所以若我們預先檢查資料是否要傳輸可以省下一些時間。
    * 若process在左邊，則若它的資料之最大值比右邊資料的最小值還小或一樣，則不需進行之後的傳遞工作。
    * 若process在右邊，則若它的資料之最小值比左邊資料的最大值還大或一樣，則不需進行之後的傳遞工作。
    * 利用MPI_Sendrecv為一種non-blocking的傳輸方式可以不用等待接收信號就進行下一步驟，可以有效節省時間。
```MPI=
// give to right
MPI_Sendrecv(buffer+data_size-1, 1, MPI_FLOAT, rank + 1, 0, rn_buffer, 1, MPI_FLOAT, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
if(buffer[data_size-1]>rn_buffer[0]){// do exchange}
// give to left
MPI_Sendrecv(buffer, 1, MPI_FLOAT, rank - 1, 0, ln_buffer+ln_data_size-1, 1, MPI_FLOAT, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
if(buffer[0]<ln_buffer[ln_data_size-1]){// do exchange}
```
* ***左右process進行資料互換*** : 先用MPI_Sendrecv進行資料互換儲存在各自的buffer中(rn_buffer或ln_buffer)，然後左右兩鄰居用比較自己每個資料的大小進行互換。
* 若process在左邊，兩鄰邊的資料從左而右看，把數字小的放到temp buffer儲存，這裡有一個條件需要判斷，有一個狀況是因為左邊proceess的資料數量一定大於等於右邊的，所以有可能發生右邊process的資料檢查到底了但temp buffer的資料數還是不夠，所以要再繼續將左邊process的資料繼續儲存在temp buffer中，然後再將temp buffer的資料跟原本自己的作互換。

```MPI=
MPI_Sendrecv(buffer, data_size-1, MPI_FLOAT, rank + 1, 0, rn_buffer+1, rn_data_size-1, MPI_FLOAT, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
int id = 0, rb_id = 0, tp_id = 0;
while (id < data_size && rb_id < rn_data_size && tp_id < data_size)
{
    if (buffer[id] < rn_buffer[rb_id])
    {
        tp_buffer[tp_id] = buffer[id];
        id++;
        tp_id++;
    }
    else
    {
        tp_buffer[tp_id] = rn_buffer[rb_id];
        rb_id++;
        tp_id++;
    }
}
if (tp_id < data_size && rb_id == rn_data_size)
{
    while (tp_id < data_size)
    {
        tp_buffer[tp_id] = buffer[id];
        tp_id++;
        id++;
    }
}
swap(tp_buffer, buffer);
```
* 若process在右邊，兩鄰邊的資料從右而左看，把數字大的放到temp buffer儲存，這裡不須條件判斷，因為左邊proceess的資料數量一定大於等於右邊的，然後再將temp buffer的資料跟原本自己的作互換。
    
```MPI=
MPI_Sendrecv(buffer+1, data_size-1, MPI_FLOAT, rank - 1, 0, ln_buffer, ln_data_size-1, MPI_FLOAT, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
int id = data_size - 1, lb_id = ln_data_size - 1, tp_id = data_size - 1;
while (id > -1 && lb_id > -1 && tp_id > -1)
{
    if (buffer[id] > ln_buffer[lb_id])
    {
        tp_buffer[tp_id] = buffer[id];
        id--;
        tp_id--;
    }
    else
    {
        tp_buffer[tp_id] = ln_buffer[lb_id];
        lb_id--;
        tp_id--;
    }
}
swap(tp_buffer, buffer);
```
* ***位置換邊*** : 依據odd-event-sort的規則，要進行基偶phase的交換，依照這邊左邊和右邊一定是互換的規則下，將process的位置移動從左到右或右到左，接著進行下一個迴圈的迭代。
```MPI=
if (position == 0)
    position = 1;
else
    position = 0;
```
8. ***寫入排序後的資料*** : 依照原本每個process自己的data_location寫入資料。
```MPI=
MPI_File_open(MPI_COMM_WORLD, output_filename, MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &output_file);
MPI_File_write_at(output_file, sizeof(float) * data_location, buffer, data_size, MPI_FLOAT, MPI_STATUS_IGNORE);
MPI_File_close(&output_file);
```
9. ***結束MPI環境*** : 
```MPI=
MPI_Finalize();
```
