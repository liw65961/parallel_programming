# NTHU CS542200 Parallel Programming Homework 2: Mandelbrot Set
## 112062591 李威辰

## Implementation
此作業要利用A. pthread和B. OMP、MPI(hybrid)此兩種方式來實作平行Mandelbrot Set的計算。
### A. pthread
1. **初始化參數**
```pthread=
//Global
unsigned long long ncpus;
int iters;
double left;
double right;
double lower;
double upper;
int width;
int height;
double now_row;
int *image;
int iter_row;
double space_row;
double space_col;
pthread_mutex_t lock;
// main
pthread_t *threads = new pthread_t[ncpus];
int *threads_arg = new int[ncpus];
image = (int *)malloc(width * height * sizeof(int));
assert(image);
pthread_mutex_init(&lock, NULL);
space_row = (upper - lower) / height;
space_col = (right - left) / width;
iter_row = 0;
```
* ncpus：CPU 數量
* iters：mandelbrot set最大迭代次數。
* left, right, lower, upper：複數平面上計算mandelbrot set定義域的範圍。
* width, height：x軸上點的數量、y軸上點的數量。
* image：用來儲存計算結果圖像的陣列。
* iter_row：追踪當前分給各threads計算的"列"。
* space_row, space_col：每個點在複數平面x軸y軸上對應的距離。
* lock：用來保護共享資源 iter_row。
* threads : threads的總數量。 
* threads_arg : threads 呼叫函數的傳遞資料。這裡紀錄每個threads自己的id。
2. **製造threads去執行mandelbrot set的任務**
```pthread=
for (int i = 0; i < ncpus; i++)
{
    threads_arg[i] = i;
    pthread_create(&threads[i], nullptr, cal_md_set, (void *)&threads_arg[i]);
}
```
* void *cal_md_set(void *arg): mandelbrot set函數。
3. **每個threads去執行Mandelbrot Set計算** : cal_md_set函數用每一個"列"去分派給每個threads計算。這裡的**加速程式方式**是:
    1. **只要一個threads做完任務(算完一列)就去接著計算下一列**。
    2. **利用AVX-512指令集**，一次處理8個數的運算，提升總體計算效率。
以下為cal_md_set函數的程式碼，分塊解釋 :
```pthread=
void *cal_md_set(void *arg)
{
    while (iter_row < height)
    {
        int local_iter_row;
        pthread_mutex_lock(&lock);
        local_iter_row = iter_row;
        iter_row++;
        pthread_mutex_unlock(&lock);

        double thread_row = lower + local_iter_row * space_row;
        int vn=8;
        int it=vn-1;
        int break_while=1;
        int iv[vn];
        for(int i=0;i<vn;i++)iv[i]=i;
        double ax0[vn];
        memset(ax0, 0, vn * sizeof(double));
        for(int i=0;i<vn;i++)ax0[i]=iv[i] * space_col + left;
        __m512d x0 = _mm512_loadu_pd(ax0);
        __m512d y0 = _mm512_set1_pd(thread_row);
        __m512d x = _mm512_setzero_pd();
        __m512d y = _mm512_setzero_pd();
        __m512d xx = _mm512_mul_pd(x, x);
        __m512d yy = _mm512_mul_pd(y, y);
        __m512d length_squared = _mm512_setzero_pd();

        int store_repeats[vn];
        memset(store_repeats, 0, vn * sizeof(int));
        
        while(break_while)
        { 
            __m512d temp = _mm512_add_pd(_mm512_sub_pd(xx, yy), x0);
            y = _mm512_add_pd(_mm512_mul_pd(_mm512_set1_pd(2.0), _mm512_mul_pd(x, y)), y0);
            x = temp;
            xx = _mm512_mul_pd(x, x);
            yy = _mm512_mul_pd(y, y);
            length_squared = _mm512_add_pd(xx, yy);

            double len[vn];
            _mm512_storeu_pd(len, length_squared);
            for(int i=0;i<vn;i++)
            {
                store_repeats[i]++;
                if (store_repeats[i] >= iters || len[i] >= 4.0)
                {
                    if (iv[i] < width) image[local_iter_row * width + iv[i]] = store_repeats[i];
                    it++;
                    iv[i]=it;
                    if (iv[i] < width)
                    {
                        x0[i]=iv[i] * space_col + left;
                        x[i]=0;
                        y[i]=0;
                        xx[i]=0;
                        yy[i]=0;
                        length_squared[i]=0;
                        store_repeats[i]=0;
                    }
                }
            }
            
            for(int i=0;i<vn;i++)
            {
                if(iv[i]<width)break;
                else if(i==vn-1)break_while=0;
            }
        }
    }
    return NULL;
}
```
* **控制mandelbrot set的迴圈計算** : while (iter_row < height)去控制mandelbrot set的迴圈計算，其中iter_row也就是下一個要分給thread的列若大於等於height高度數量邊界上線就會跳出迴圈不執行並返回。
* **限制共用參數iter_row存取** : 由於參數iter_row是用來分配給每個threads的執行"列"，所以每個thread都會操作並存取到，要用互斥存取。
```pthread=
int local_iter_row;
pthread_mutex_lock(&lock);
local_iter_row = iter_row;
iter_row++;
pthread_mutex_unlock(&lock);
```
local_iter_row : iter_row分給執行thread的"列"。每次存取完iter_row要加一跑到往上一列再給下一個thread。
* **一些參數設定** : 
```pthread=
double thread_row = lower + local_iter_row * space_row;
int vn=8;
int it=vn-1;
int break_while=1;
int iv[vn];
for(int i=0;i<vn;i++)iv[i]=i;
double ax0[vn];
memset(ax0, 0, vn * sizeof(double));
for(int i=0;i<vn;i++)ax0[i]=iv[i] * space_col + left;
__m512d x0 = _mm512_loadu_pd(ax0);
__m512d y0 = _mm512_set1_pd(thread_row);
__m512d x = _mm512_setzero_pd();
__m512d y = _mm512_setzero_pd();
__m512d xx = _mm512_mul_pd(x, x);
__m512d yy = _mm512_mul_pd(y, y);
__m512d length_squared = _mm512_setzero_pd();

int store_repeats[vn];
memset(store_repeats, 0, vn * sizeof(int));
```
vn : 向量化處理參數的數量，此處用AVX-512指令集，這裡的參數適用64位元的double，所以512/64=8，所以一個向量中elements的數量為vn = 8。
it : 該"列"的計算到第幾"行"數，由於初始已經分配給0到7，所以it=7，下面it++後從8開始。
break_while : 當break_while=0，下方利用迴圈計算每一"列"的"行"的迴圈會中止。
iv[] : 儲存運算向量中每個element所對應到的"列"。
ax0[] : 儲存運算向量中的x座標給x0做初始化。
x0 : 運算向量中的x座標。
y0 : 運算向量中的y座標。
x : 運算中x座標的暫存器。
y : 運算中y座標的暫存器。
xx : 運算x * x的暫存器(避免一直重複計算浪費時間)。
yy : 運算y * y的暫存器(避免一直重複計算浪費時間)。
length_squared : 儲存運算的最終結果。
store_repeats[] : 儲存運算向量中各elements座標的計算iteration次數。
* **計算mandelbrot set** : 將範例sequential版計算mandelbrot set的公式修改成可以套用此方法的平行運算版本。
```pthread=
while(break_while)
{ 
    __m512d temp = _mm512_add_pd(_mm512_sub_pd(xx, yy), x0);
    y = _mm512_add_pd(_mm512_mul_pd(_mm512_set1_pd(2.0), _mm512_mul_pd(x, y)), y0);
    x = temp;
    xx = _mm512_mul_pd(x, x);
    yy = _mm512_mul_pd(y, y);
    length_squared = _mm512_add_pd(xx, yy);

    double len[vn];
    _mm512_storeu_pd(len, length_squared);
    for(int i=0;i<vn;i++)
    {
        store_repeats[i]++;
        if (store_repeats[i] >= iters || len[i] >= 4.0)
        {
            if (iv[i] < width) image[local_iter_row * width + iv[i]] = store_repeats[i];
            it++;
            iv[i]=it;
            if (iv[i] < width)
            {
                x0[i]=iv[i] * space_col + left;
                x[i]=0;
                y[i]=0;
                xx[i]=0;
                yy[i]=0;
                length_squared[i]=0;
                store_repeats[i]=0;
            }
        }
    }

    for(int i=0;i<vn;i++)
    {
        if(iv[i]<width)break;
        else if(i==vn-1)break_while=0;
    }
}
```
* 下列程式是做8個元素的向量化mandelbrot set計算並將結果存於length_squared然後傳給double type的陣列len。
```pthread=
__m512d temp = _mm512_add_pd(_mm512_sub_pd(xx, yy), x0);
y = _mm512_add_pd(_mm512_mul_pd(_mm512_set1_pd(2.0), _mm512_mul_pd(x, y)), y0);
x = temp;
xx = _mm512_mul_pd(x, x);
yy = _mm512_mul_pd(y, y);
length_squared = _mm512_add_pd(xx, yy);

double len[vn];
_mm512_storeu_pd(len, length_squared);
```
* 下列程式是在檢查向量中每個元素座標點的運算是否達到中止條件，上面計算完一次所以store_repeats++，然後若符合計算次數小於iters且計算結果比4.0小的就繼續下一輪計算，否則看向量中該元素的座標"行"是否在規定的範圍內(iv[i] < width)，在範圍內就儲存給image[]，然後it++(行+1)和iv[i]=it。由於此點進來這個判斷式代表計算中止所以要算下一行，須將一些運算參數再做初始化(若iv[i] >= width則不需再作該新座標的運算，保留原本舊的，不須初始化)。利用迴圈檢查向量中每個element的計算結果去判斷接下來的任務，若**完成會直接初始化並指派下一個座標點的計算任務**，**讓運算不停滯提升整體效能**。
```pthread=
for(int i=0;i<vn;i++)
{
    store_repeats[i]++;
    if (store_repeats[i] >= iters || len[i] >= 4.0)
    {
        if (iv[i] < width) image[local_iter_row * width + iv[i]] = store_repeats[i];
        it++;
        iv[i]=it;
        if (iv[i] < width)
        {
            x0[i]=iv[i] * space_col + left;
            x[i]=0;
            y[i]=0;
            xx[i]=0;
            yy[i]=0;
            length_squared[i]=0;
            store_repeats[i]=0;
        }
    }
}
```
下列程式用來控制迴圈是否繼續，若向量中的所有點座標都超出範圍，則結束迴圈break_while=0。
```pthread=
for(int i=0;i<vn;i++)
{
    if(iv[i]<width)break;
    else if(i==vn-1)break_while=0;
}
```
4. **回收執行完成threads** : 利用pthread_join來回收子thread和其暫用資源並且破壞控制互斥存取參數的lock。
```pthreads=
for (int i = 0; i < ncpus; i++)
{
    pthread_join(threads[i], NULL);
}
pthread_mutex_destroy(&lock);
```
5. **畫圖和釋放image[]記憶體**
```pthread=
write_png(filename, iters, width, height, image);
free(image);
```
### B. MPI + OpenMP(Hybrid)
1. **初始化參數** : 初始化參數和MPI，後續可以呼叫MPI函數。
```Hybrid=
/* argument parsing */
assert(argc == 9);
const char* filename = argv[1];
int iters = strtol(argv[2], 0, 10);
double left = strtod(argv[3], 0);
double right = strtod(argv[4], 0);
double lower = strtod(argv[5], 0);
double upper = strtod(argv[6], 0);
int width = strtol(argv[7], 0, 10);
int height = strtol(argv[8], 0, 10);

/* allocate memory for image */
int* image = (int*)malloc(width * height * sizeof(int));
int* total_image = (int*)malloc(width * height * sizeof(int));
memset(image, 0, width * height * sizeof(int));
memset(total_image, 0, width * height * sizeof(int));
assert(image);

int rank, size;
MPI_Init(&argc, &argv);
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
MPI_Comm_size(MPI_COMM_WORLD, &size);
double space_row=(upper - lower) / height;
double space_col=(right - left) / width;
```
* iters：mandelbrot set最大迭代次數。
* left, right, lower, upper：複數平面上計算mandelbrot set定義域的範圍。
* width, height：x軸上點的數量、y軸上點的數量。
* image：用來儲存各process計算結果圖像的陣列。
* total_image : 用來儲存所有process計算結果圖像的陣列。
* space_row, space_col：每個點在複數平面x軸y軸上對應的距離。
* rank：目前執行中process的編號，每個都會有一個唯一的rank。
* size：processes的總數。
2. **執行Mandelbrot Set計算** : 這裡利用依照每個**process的rank去分配工作**，從第rank"列"開始，然後依序rank + size、rank + 2 * size...，用for迴圈去跑迭代，然後外面在掛上**OMP dynamic的parallel for去再將此迴圈平行化去加速計算過程**。然後利用**AVX-512指令集去加速計算過程**，進入迴圈之後的計算基本上和pthread一模一樣這裡就不多贅述。

```Hybrid=
/* mandelbrot set */
#pragma omp parallel for schedule(dynamic)
for (int j = rank; j < height; j+=size) {
    int vn=8;
    int it=vn-1;
    int break_while=1;
    int iv[vn];
    for(int i=0;i<vn;i++)iv[i]=i;
    double ax0[vn];
    memset(ax0, 0, vn * sizeof(double));
    for(int i=0;i<vn;i++)ax0[i]=iv[i] * space_col + left;
    __m512d x0 = _mm512_loadu_pd(ax0);
    __m512d y0 = _mm512_set1_pd(j * space_row + lower);
    __m512d x = _mm512_setzero_pd();
    __m512d y = _mm512_setzero_pd();
    __m512d xx = _mm512_mul_pd(x, x);
    __m512d yy = _mm512_mul_pd(y, y);
    __m512d length_squared = _mm512_setzero_pd();
    int store_repeats[vn];
    memset(store_repeats, 0, vn * sizeof(int));

    while(break_while)
    { 
        __m512d temp = _mm512_add_pd(_mm512_sub_pd(xx, yy), x0);
        y = _mm512_add_pd(_mm512_mul_pd(_mm512_set1_pd(2.0), _mm512_mul_pd(x, y)), y0);
        x = temp;
        xx = _mm512_mul_pd(x, x);
        yy = _mm512_mul_pd(y, y);
        length_squared = _mm512_add_pd(xx, yy);

        double len[vn];
        _mm512_storeu_pd(len, length_squared);
        for(int i=0;i<vn;i++)
        {
            store_repeats[i]++;
            if (store_repeats[i] >= iters || len[i] >= 4.0)
            {
                if (iv[i] < width) image[j * width + iv[i]] = store_repeats[i];
                it++;
                iv[i]=it;
                if (iv[i] < width)
                {
                    x0[i]=iv[i] * space_col + left;
                    x[i]=0;
                    y[i]=0;
                    xx[i]=0;
                    yy[i]=0;
                    length_squared[i]=0;
                    store_repeats[i]=0;
                }
            }
        }

        for(int i=0;i<vn;i++)
        {
            if(iv[i]<width)break;
            else if(i==vn-1)break_while=0;
        }
    }
}
```
3. **集中結果和MPI終止** : 將答案存於total_image和結束MPI。
```Hybrid=
MPI_Reduce(image, total_image, height*width, MPI_INT, MPI_BOR, 0, MPI_COMM_WORLD);
MPI_Barrier(MPI_COMM_WORLD);
MPI_Finalize();
```
4. **輸出和釋放記憶體** : 由rank=0之process做輸出和釋放total_image、image陣列的空間。
```Hybrid=
if (rank == 0) 
{
    /* draw and cleanup */
    write_png(filename, iters, width, height, total_image);
}
free(total_image);
free(image);
```
## Experiment & Analysis
這裡在QCT CPU平台做測試，用nsys做profiling，用nvtxpush、pop計算時間。
在計算的迴圈上下加上nvtxpush、pop。
以下討論大多以computation time為主，然後因為MPI message passing的特性，最終結果輸出只交給rank0，所以IO time在Hybrid中不探討。
1. **Scalability** : 這裡探討程式在threads或process越多的情形下表現是否好(執行速率有良好的倍數成長):

pthread:
testcase: strict36
thread數量: 1, 4, 12, 24
![pthread-speedup,strict36](https://hackmd.io/_uploads/BJPzwZz-Jx.png)
testcase: strict20
thread數量: 1, 4, 12, 24
![pthread-speedup,strict20](https://hackmd.io/_uploads/SJEQLGMbJe.png)
computation time可以看到各點幾乎都有符合線性增長。pthread的I/O time不太會因為threads增加而減少(可能為硬體設備以及網路因素)，導致整體的時間加速不明顯(趨於平緩)。

Hybrid:
(固定CPU)
testcase: strict36
各process cpu數量: 2
process數量: 1, 4, 12, 24
![hybrid-speedup, strict36, 2 cpu](https://hackmd.io/_uploads/rJQwx-WbJl.png)
(固定CPU)
testcase: strict20
各process cpu數量: 2
process數量: 1, 4, 12, 24
![hybrid-speedup, strict20, 2 cpu](https://hackmd.io/_uploads/SyLrIfzZ1x.png)
可以看到各點幾乎都有符合線性增長，只有process=24時可能有因為process數量較多有較多的時間處裡其他資料問題而變慢，沒有達到預期的6倍結果。

(固定process)
testcase: strict36
各process cpu數量: 4, 8, 12, 24
process數量: 4
![hybrid-speedup, strict36, 4 process](https://hackmd.io/_uploads/r1v1XMbZJx.png)
(固定process)
testcase: strict20
各process cpu數量: 4, 8, 12, 24
process數量: 4
![hybrid-speedup, strict20, 4 process](https://hackmd.io/_uploads/ryywIfzWke.png)

可以看到各點幾乎都沒有符合線性增長，效果都不如預期，有可能是單個process的CPU(threads)太多造成運算傳遞資料等負荷量太大，使時間變慢。

2. **loadbalance** :
此處探討process或threads之間的工作是否分布平均。
pthread:
testcase: strict36
thread數量:24
標準差 : 0.005245 
![pthread-loadbalance,strict36 , thread24](https://hackmd.io/_uploads/SytT-W-WJe.png)
由此圖和標準差發現，這個程式的工作分配的滿均勻的，標準差小，沒有threads的computation time特別多或少。
Hybrid:
testcase: strict36
各process cpu數量: 2
process數量: 12
標準差: 0.004405496![hybrid-loadbalance,strict36 ,cpu 2, process12](https://hackmd.io/_uploads/Bk8chbMbJx.png)
testcase: strict36
各process cpu數量: 4
process數量: 6
標準差 : 0.000871551
![hybrid-loadbalance,strict36 ,cpu 4, process 6](https://hackmd.io/_uploads/Bkjo3ZM-kg.png)
由此圖和標準差發現，這個程式的工作分配的滿均勻的，標準差小，沒有process的computation time特別多或少。
3. **程式優化比較**:
這裡比較一下有使用AVX512 vectorized優化計算的程式和沒有使用的，看computation速度差在哪裡。
pthread:
testcase: strict36
thread數量: 1, 4, 12, 24
![pthread-speedup, strict36 , no-512](https://hackmd.io/_uploads/rynuKbZbkx.png)
Hybrid:
testcase: strict36
各process cpu數量: 2
process數量: 1, 4, 12, 24
![hybrid-speedup, strict36 , no-512](https://hackmd.io/_uploads/BkGjK-W-kg.png)
這裡可以看到pthread和Hybrid的speedup差不多在4.多出頭，並沒有達成預期整個計算時間變為原本的1/8，可能是程式優化的不完全或是在資料運算的時間有一些等待例如mutex_lock等待時間，所以才沒有達成預期的speedup。


## conclusion
這次讓我學習到傳統pthread的運作和程式實作方法，以及Hybrid(MPI+OMP)的實作方法及注意細節，而且是第一次碰觸到將資料向量化做運算的方法，覺得非常有趣。此次作業的難點其實主要是實作pthread的程式和將資料向量化做運算實作，因為這兩個都很多東西都要自己去手刻和設定參數，然後pthread還有racecondition的問題，所以在實作這兩件事情有遇到不少細節的問題，但還好最後的結果speedup還算滿意有符合預期，所以這次作業整體還算做的開心。






